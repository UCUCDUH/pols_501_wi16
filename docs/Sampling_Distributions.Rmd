```{r results='hide',echo=FALSE}
knitr::opts_chunk$set(cache = TRUE, autodep = TRUE)
```


```{r}
library("dplyr")
library("ggplot2")
```



# Inference

If we knew the population distribution, we can calculate the probability of observing a sample statistic using the sampling distribution.

- Known population: distribution and parameters. e.g. $N(\mu, \sigma)$
- Draw samples. $X \sim N(\mu, \sigma)$
- Calculate sample statistics which have a sampling distribution, $\bar{X} \sim N(\mu, \sigma / \sqrt{n})$

However, in inference we only know the sample or sample statistic, and want to know some parameter(s) of the population distribution.

- Known sample or at least a known statistic of a sample. E.g. $\bar{x}$.
- ?
- Population parameter: E.g. $\mu$.

The obvious way to do this would be to use Bayes' Theorem,
$$
p(\mu | \bar{x}) = \frac{\Pr(\bar{x}| \mu) \Pr(\mu)}{\Pr(\bar{x})}
$$
There are at least two problems with this approach

1. Practically, this can be computationally difficult
2. Theoretically, we need to assign a probability to the population parameter, prior to observing the data, $\Pr(\mu)$.
   This may be objectionable to people.
3. In the frequentist definition of probability, there is no probability of $\mu$. Parameters are not random variables, they are fixed, though unknown, values. Only samples are random variables, and can have probabilities. Therefore, we can calculate the probability of observing a sample mean, $\bar{x}$, in repeated samples given a population mean, $\mu$. However, there is no non-degenerate probability for the population parameter $\mu$.

There are two main frequentist approaches to the inferential problem:

 1. Confidence intervals
 2. Hypothesis Testing: Calculate the sampling distribution of the sample statistic given a null hypothesis, $H_a$, and calculate how unusual the data we observe is.

     1. P-value
     2. Significance test

These methods represent distinct and separate methods of inference, although in practice they are used in a hybrid, and somewhat incoherent, manner. 

# Sampling distribution

The sampling distribution of a sample statistic depends is the distribution of a sampling statistic in repeated samples.


We'll illustrate the sampling distribution of the mean from a population distributed standard normal.
To draw a single sample of size 8 and calculate the mean,
```{r}
n <- 8
mean(rnorm(n))
```
However, to get a sample from the sampling distribution, we need to draw many samples and for each sample calculate the mean.
```{r}
n <- 8
iter <- 2048
# Investigate
results <- list()
for (i in 1:iter) {
  x <- rnorm(n, mean = mu, sd = sigma)
  stat <- mean(x)
  results[[i]] <- data_frame(i = i, stat = mean(x))
}
results <- bind_rows(results)
```
Now we can plot the results,
```{r}
ggplot(results, aes(x = stat)) +
  geom_density() +
  geom_rug()
```
Try the code above by with different sample sizes: 2, 8, 16, 32, 512, 1024.

We can generalize the above code to run multiple sample sizes and plot them all.
Sample sizes to draw,
```{r}
sample_sizes <- 2 ^ (0:10)
```
Number of samples to draw from the sampling distribution for each sample size,
```{r}
iter <- 4096
```
For each sample size, draw samples from the sampling distribution,
```{r}
results <- list()
for (k in seq_along(sample_sizes))  {
  cat("sample size:", sample_sizes[k], "\n")
  sampling_dist <- list()
  n <- sample_sizes[k]
  for (i in 1:iter) {
    # Normal distribution. mean = 0, sd = 1. mean = 0
    x <- rnorm(n)
    # Uniform distribution: min = 0, max = 1. mean = 0.5
    # x <- runif(n)
    # Geometric distribution: prob = 0.25. mean = 4
    # x <- rgeom(n, prob = 0.25)
    # Beta distribution. Bimodal. mean = 0.5.
    # x <- rbeta(n, shape1 = 0.25, shape2 = 0.25)

    # Statistics
    # # Mean
    # stat <- mean(x)
    # # Median
    # stat <- median(x)
    # # Maximum
    # stat <- max(x)
    # # Standard deviation
    # stat <-sd(x)
    # # Geometric mean of
    # stat <- exp(mean(log(abs(x))))
    sampling_dist[[i]] <- data_frame(size = n, stat = stat)
  }
  results[[k]] <- bind_rows(sampling_dist)
}
results <- bind_rows(results) %>%
  mutate(size = factor(size))
```
Plot the results for different sample size,
```{r}
ggplot(results, aes(x = stat, colour = size)) +
  geom_density()
```

In general the sampling distribution depends on three things:

- the population distribution
- the sample size
- the sample statistic

In the code there are alternative statistics and distributions, uncomment 
the code to try different combinations and see how the affect the shape of the
sampling distribution.


# Sampling Distribution of the Sample Mean

The sampling distribution of the sample mean has the following properties

- Mean: $\mu$
- Standard deviation: $\sigma / \sqrt{n}$
- As $n \to \infty$, the distribution approaches a normal distribution (regardless of the population distribution[^1]). 

The first two properties are known as the Law of Large Number (LLN), and the final property is called the Central Limit Theorem (CLT).

Let's convince ourselves that these hold by simulating means from a variety of population distributions, and plotting the distribution of the sampling distribution, its mean, and its standard deviation.

```{r message=FALSE}
## Comparing mean and sample distributions of multiple samples
iter <- 8192
sample_sizes <- 2 ^ (0:12)
results <- list()
for (k in seq_along(sample_sizes))  {
  n <- sample_sizes[k]
  message("sample size: ", n)
  stat <- rep(NA, iter)
  for (i in 1:iter) {
    # # Normal distribution.
    # # mean = 0, sd = 1
    #x <- rnorm(n)
    
    # # Uniform distribution:
    # # min = 0, max = 1, mean = 0.5, sd = 0.2887
    # x <- runif(n)
    
    # # Geometric distribution: prob = 0.25.
    # # mean = (1 - p) / p = 3
    # # var = (1 - p) / p ^ 2 = 12
    # # sd = sqrt(12)
    # x <- rgeom(n, prob = 0.25)
    
    # # Beta distribution. Bimodal.
    # # mean = 0.5. sd = 0.4082
    # x <- rbeta(n, shape1 = 0.5, shape2 = 0.5)

    # # Bernoulli distribution.
    # # mean = 0.2. sd = sqrt(0.2 * 0.8)
    x <- rbinom(n, size = 1, prob = 0.2)
    
    stat[i] <- mean(x)
  }
  results[[k]] <- data_frame(stat = stat, size = n)

}
results <- bind_rows(results)

results_summary <- results %>%
  group_by(size) %>%
  summarize(x_mean = mean(stat),
            s = sd(stat),
            x_mean_se = s / sqrt(iter),
            s_se = s / sqrt(2 * (iter - 1)))
```

Set the population mean and standard deviation of the population distribution that
you chose,
```{r}
mu <- 0
sigma <- 1
```

Plot the mean for different sample sizes. The mean of the sampling distribution is equal to the population mean, $\mu$.
```{r}
ggplot(results_summary, aes(x = size, y = x_mean, 
                    ymin = x_mean - 2 * x_mean_se,
                    ymax = x_mean + 2 * x_mean_se)) +
  geom_pointrange() +
  geom_hline(yintercept = mu) +
  scale_x_log10()
```

Plot of the standard error for different sample sizes. The standard error of the sample mean is $\sigma / \sqrt{n}$.
```{r}
ggplot(results_summary, aes(x = size,
                    y = s, 
                    ymin = s - 2 * s_se,
                    ymax = s + 2 * s_se)) +
  geom_pointrange() +
  stat_function(fun = function(n) sigma / sqrt(n)) +
  scale_x_log10()
```

Plot of the distribution of the sampling distribution of the mean as it increases.
```{r}
ggplot(mutate(results,
              normal = dnorm(stat, mean = mu, sd = sigma / sqrt(size))),
       aes(x = stat)) +
  geom_density() +
  geom_rug() +
  geom_line(mapping = aes(y = normal), col = "red") +
  facet_wrap(~size, ncol = 2, scale = "free")
```

[^1]: There are a few technical conditions for the CLT, but that's not our concern here.

# Binomial Distribution and the Bernoulli Distribution

The sampling distribution of the sum of Bernoulli distributed random variables is the binomial distribution.
As the sample size increases, the Binomial distribution approaches a normal distribution with mean $n p$ and standard deviation $\sqrt{n p (1 - p)}$.

```{r}
prob <- 0.2
sizes <- c(1, 2, 8, 32, 64, 256, 512, 1024)

binomial <- list()
for (i in seq_along(sizes)) {
  n <- sizes[i]
  binomial[[i]] <- data_frame(x = 0:n,
                              size = n,
                              p = dbinom(x, size = n, prob = prob))
}
binomial <- bind_rows(binomial)

normal <- list()
for (i in seq_along(sizes)) {
  n <- sizes[i]
  normal_mean <- n * prob
  normal_sd <- sqrt(n * prob * (1 - prob))
  normal[[i]] <- data_frame(x = seq(normal_mean - 3 * normal_sd,
                                    normal_mean + 3 * normal_sd, 
                                    length.out = 101),
                            y = dnorm(x, mean = normal_mean, sd = normal_sd),
                            size = n)
}
normal <- bind_rows(normal)

ggplot() +
  geom_bar(data = binomial, aes(x = x, y = p), stat = "identity",
           alpha = 0.5) +
  geom_line(data = normal, aes(x = x, y = y), color = "black", size = 1) +
  facet_wrap(~size, ncol = 2, scales = "free")
```


# Evaluating Estimators

Estimand

:   Parameter to be estimated. E.g. population mean, population variance.

Estimator

:   A rule for calculating an estimate given data. E.g. sample mean, sample variance.

Estimate

:   A particular value of the estimator applied to data. E.g. 4.

We evaluate estimators, not particular estimates.
We want to judge methods based on how well they work in repeated samples.

# How can we evaluate estimators?

We need to see how well they do in repeated samples, e.g. their sampling distribution.
Let $\theta$ be the true value of the parameter, and $\hat\theta$ be an estimator for that parameter.
Note that while $\theta$ is fixed, $\hat\theta$ is a random variable.

## Bias

The bias of an estimator is the difference between the expected value of its sampling distribution and the true value of the parameter,
$$
Bias(\theta) = \E(\hat\theta) - \theta .
$$

## Variance

The variance of an estimator is the variance of its sampling distribution,
$$
\var(\hat\theta)  = \E\left( (\hat\theta - \E(\hat\theta))^2 \right) .
$$

## MSE

The mean squared error evaluates the distribution on its squared error relative to the true value of the parameter,
$$
MSE(\hat\theta) &= \E\left[\left(\theta(X) - \theta\right)^2 \right] .
$$
The MSE incorporates both the bias of the estimator and the variance, and can be rewritten as the sum of the variance and bias squared of the estimator,
$$
MSE(\hat\theta) = \var(\hat\theta) + \left(Bias(\hat\theta)\right)^2
$$
One implication of this is that biased esimators may have a better MSE than a unbiased estimator, if their variance is sufficiently lower,
$$
\var(\hat\theta_{\text{unbiased}}) - var(\hat\theta_{\text{biased}}) > {Bias(\hat\theta)}^2 .
$$

*What are the bias, variance, and MSE of the sample mean as an estimator of the population mean?*

Its bias is 0, since $E(\bar{X}) = \mu$.
Its variance is $\sigma^2 / n$.
Since the sample mean is unbiased, its MSE is the same as its variance
$$
MSE(\bar{X}) = \var(\bar{X}) + \left(Bias(\bar{X})\right)^2 = \var(\bar{X}) + 0 = \var(\bar{X})
$$


## Other criteria

Estimators can be evaluated according to other criteria. A few common ones are:

Consistency

:   As the sample size increases ($n \to infty$) the estimator $\hat\theta$ gets arbitrarily close to the true value of the estimand $\theta$.

Efficiency

:   For unbiased estimators of an estimand, the estimator with the lowest variance.

Robustness:

:   The estimate is not affected much by departures from the assumptions, or the estimator requires few assumptions.


# Confidence Intervals



```{r}
# calculate z critical value
alpha <- 1 - (confidence / 100)
z <- -qnorm(alpha / 2)
results <- list()
for (i in 1:iter) {
  x <- rnorm(n, mu, sigma)
  x_mean <- mean(x)
  s <- sd(x)
  se <- sd(x) / sqrt(n)
  lb <- x_mean - z * se
  ub <- x_mean + z * se
  contains <- (mu > lb) & (mu < ub)
  results[[i]] <- data_frame(x_mean = x_mean,
                             s = s,
                             se = se,
                             lb = lb,
                             ub = ub,
                             contains = contains)
}
results <- bind_rows(results)
```



# Significance Tests

p-value vs. significance level

| Hypothesis | True | False | 
| True | | Type I |
| False | Type II | |


```{r}

null_hypothesis <- 0
mu_values <- c(0, 0.125, 0.25, 0.5, 1, 2)
sigma <- 1
sample_sizes <- c(8, 16, 32, 64, 128, 256)
iter <- 4096

results <- vector(mode = "list",
                  length = length(sample_sizes) *
                  length(mu_values) *
                    iter)
result_num <- 1
for (i in seq_along(mu_values)) {
  mu <- mu_values[i]
  for (j in seq_along(sample_sizes)) {
    size <- sample_sizes[j]
    cat("mu:", mu, ", size:", size, "\n")
    for (k in 1:iter) {
      x <- rnorm(size, mean = mu, sd = sigma)
      x_mean <- mean(x)
      s <- sd(x)
      # s <- sigma
      se <- s / sqrt(size)
      z <- (x_mean - null_hypothesis) / se
      p_value <- 2 * pnorm(-abs(z))
      results[[result_num]] <-
        data_frame(size = size,
                   mu = mu,
                   x_mean = x_mean,
                   s = s,
                   se = se,
                   z = z,
                   p_value = p_value,
                   same_sign = sign(x_mean) == sign(mu),
                   effect = x_mean - mu
                   )
       result_num <- result_num + 1
    }
  }
}
results <- bind_rows(results)

errors <- 
  results %>%
    group_by(mu, size) %>%
    mutate(decision_error = ifelse(mu == null_hypothesis, p_value < 0.05, p_value > 0.05),
           magnitude_error = ifelse(p_value < 0.05, abs(x_mean - mu), NA),
           sign_error = sign(x_mean) != sign(mu)) %>%
    summarize(decision_error = sum(decision_error) / length(decision_error),
              magnitude_error = mean(magnitude_error, na.rm = TRUE),
              sign_error = sum(sign_error) / length(sign_error)) %>%
  ungroup()

ggplot(errors %>% mutate(mu = factor(mu)),
       aes(x = sqrt(size), y = decision_error)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
  facet_wrap(~mu, ncol = 1)

ggplot(errors %>% mutate(mu = factor(mu)),
       aes(x = sqrt(size), y = sign_error)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(limits = c(0, 1)) +
  facet_wrap(~mu, ncol = 1)
  
ggplot(errors %>% mutate(mu = factor(mu)),
       aes(x = sqrt(size), y = magnitude_error)) +
  geom_point() +
  geom_line() +
  facet_wrap(~mu, ncol = 1)
  
res <- mutate(results,
              size = factor(size,
                            levels = c("8", "16", "32", "64",
                                       "128", "256", "512", "1024", "2048")),
              mu = as.character(mu))
ggplot(res,
       aes(x = p_value)) +
  geom_histogram(binwidth = 0.05, mapping = aes(y = ..ndensity..)) +
  facet_grid(size ~ mu, scale = "free_y")

ggplot(filter(res, p_value < 0.05),
       aes(x = x_mean)) +
  geom_density(mapping = aes(y = ..scaled..), fill = "black") +
  facet_grid(size ~ mu, scale = "free_y")

ggplot(mutate(res, sig = p_value < 0.05),
       aes(x = x_mean, fill = sig, color = sig)) +
  geom_histogram(binwidth = 0.1, alpha = 0.5) +
  geom_rug() +
  facet_grid(size ~ mu, scale = "free_y")

```


Some notes on Type I and Type II errors:

- For a given test, there is is tradeoff of Type I and Type II error. The fewer false positives, the more false negatives. E.g. in trial a judge could minimize any innocent defendents being declared guilty by ruling not guilty on all trials. However, then all guilty defendents would be declared not guilty.
- Tests generally focus on Type I error, and then for a given Type I error, more powerful tests are preferred. One reason to focus on Type I error, is that Type II error requires specifying a value of the alternative hypothesis, but there is often not just one value.
- Type I error is independent of sample size
- Type II decreases with sample size


- For a fixed sample mean and standard deviation: How would the significance change with the sample size?
- For a fixed sample mean and sample size: How would the significance change with the sample standard deviation?

# Frequentist vs. Bayesian Interpretations

$p(D | H)$, and $p(H | D) \propto p(D | H) p(H)$

# Ways that the estimate can break

- finite population
- correlated errors

    - serial correlation
    - cluster correlation

- skewed distribution
