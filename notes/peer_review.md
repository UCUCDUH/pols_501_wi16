#

1. In a paragraph, summarize the paper. What is the question? What were the methods?
   What were the results?

2.

3.

esearch question and hypothesis:  
 Is the researcher focused on well‐defined questions?  
 Is the question interesting and important?  
 Are the propositions falsifiable?  
 Has the alternative hypothesis been clearly stated?
 Is the approach inductive, deductive, or an exercise in data mining? Is this the right structure?
Research design:  
 Is the author attempting to identify a causal impact?  
 Is the “cause” clear? Is there a cause/treatment/program/fist stage?
 Is the relevant counterfactual clearly defined? Is it compelling?
 Is the method for doing so clear and compelling? Has statistical inference been confused with causal
inference?  
 Does the research design identify a very narrow or a very general source of variation?
 Could the question be addressed with another approach?
 Useful trick: ask yourself, “What experiment would someone run to answer this question?”
Theory/Model:  
 Is the theory/model clear, insightful, and appropriate?
 Could the theory benefit from being more explicit, developed, or formal?  
 Are there clear predictions that can be falsified? Are these predictions “risky” enough? Does the theory
generate any prohibitions that can be tested?
 Would an alternative theory/model be more appropriate?  
 Could there be alternative models that produce similar predictions—that is, does evidence on the
predictions necessarily weigh on the model or explanation?
 Is the theory a theory, or a list of predictions?
 Is the estimating equation clearly related to or derived from the model?  
Data:  
 Are the data clearly described?  
 Is the choice of data well‐suited to the question and test?  
 Are there any worrying sources of measurement error or missing data? Are any proxies reasonable?
 Are there sample size or power issues?  
 Could the data sources or collection method be biased?  
 Are there better sources of data that you would recommend?  
 Are there types of data that should have been reported, or would have been useful or essential in the
empirical analysis?  
Empirical analysis:  
 Are the statistical techniques well suited to the problem at hand?  
 What are the endogenous and exogenous variables?  
 Has the paper adequately dealt with concerns about measurement error, simultaneity, omitted variables,
selection, and other forms of bias and identification problems?  
 Is there selection not just in who receives the “treatment”, but in who we observe, or who we measure?
 Is the empirical strategy convincing?  
 Could differencing, or the use of fixed effects, exacerbate any measurement error?
 Did the author make any assumptions for identification (e.g. of distributions, exogeneity, etc)?  
 Were these assumptions tested and, if not, how would you test them?  
 Are the results demonstrated to be robust to alternative assumptions?  
 Does the disturbance term have an interpretation, or is it just tacked on?  
 Are the observations i.i.d., and if not, have corrections to the standard errors been made?  
 What additional tests of the empirical strategy would you suggest for robustness and confidence in the
research strategy?  
 Are there any dangers in the empirical strategy (e.g. sensitivity to identification assumptions)?  
 Can you imagine a better, or alternative, empirical strategy?  
Results:  
 Do the results adequately answer the question at hand?  
 Are the conclusions convincing? Are appropriate caveats mentioned?  
 What variation in the data identifies the elements of the model?  
 Are there alternative explanations for the results, and can we test for them?  
 Could the author have taken the analysis further, to look for impact heterogeneity, for causal
mechanisms, for effects on other variables, etc?
 Is absence of evidence confused with evidence of absence?
Scope:  
 Can we generalize these results?  
 Has the author specified the scope conditions?  
 Have casual mechanisms been explored?  
 Are there further types of analysis that would illuminate the external validity, or the causal mechanism at
work?  
 Are there other data or approaches that would complement the current one?

# Replicability

1. Did they cite the sources of the data? They should provide enough information so that
   you can find the original data that they used; verify that by trying to find the data.
1. Are you able to run the code?
2. Did the code produce the results that were provided in the
3. Is the code clear and accessible? In particular, can you tell which parts of
   the code produce which tables, figures, and results.

If you are having problems with the code, contact the author to resolve them.
You can cc the instructors on the email so they can assist you.

How to do peer review well (https://stat545-ubc.github.io/peer-review02_peer-evaluation-guidelines.html)

- Give thoughtful, constructive and considerate comments.
- Be specific and concise.
- Try to learn something new and, if you succeed, point that out.
- If you can’t find anything to praise or that you found helpful, then at least offer some suggestions in a kind way.

# Evaluations of Peer Review

It’s binary. Each peer review will be deemed “good” or “needs more”.

Hallmarks of “needs more” feedback:

- Your review is so generic that it’s hard to determine which assignment you’re reviewing.
- Your review is mean.
- You can’t find anything to praise/learn and yet you don’t offer any suggestions either.
- Each review you do will get you 1 point if “good” and 0 points if “needs more”.

# References

There are some good sources and suggestions on "doing" peer review:

- *The Political Methodologist*, Fall 2015, issue on Peer Review [[URL]](https://thepoliticalmethodologist.files.wordpress.com/2016/02/tpm_v23_n1.pdf), especially Brendan Nyhan, "A Checklist Manifesto for Peer Review" and Thomas Pepinsky, "What is Peer Review For? Why Referees are not the Disciplinary Police".
- Miller, Beth, Jon Pevehouse, Ron Rogowski, Dustin Tingley, and Rick Wilson. 2013. “How To Be a Peer Reviewer: A Guide for Recent and Soon-to-be PhDs.” *PS: Political Science & Politics* [[DOI]](https://dx.doi.org/10.1017/S104909651200128)
- William Thompson, *A Guide for the Young Economist*, 2nd ed. Ch 4 "Writing Referee Reports".
- Chris Blattman [Research Design & Casual Inference Syllabus](http://chrisblattman.com/files/2009/07/PLSC508-Syllabus-Spring2010.pdf), last two pages
- Kwan Choi [Being a good referee](http://www.roie.org/how.htm) (A little dated)
- Tyler Cowen [How to be a good referee](http://marginalrevolution.com/marginalrevolution/2006/10/how_to_be_a_goo.html)
- David McKenzie [How much to referee and how to do it?](http://blogs.worldbank.org/impactevaluations/how-much-to-referee-and-how-to-do-it)
- [Q&A with Larry Katz, editor of QJE](http://blogs.worldbank.org/impactevaluations/qa-with-larry-katz-editor-of-qje)

Derived in part from Jenny Bryan [Guidelines for peer review of assignments](https://stat545-ubc.github.io/faq.html) [CC BY-NC 3.0 Create Commons License](http://creativecommons.org/licenses/by-nc/3.0/)
