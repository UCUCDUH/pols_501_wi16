---
title: "Studentâ€™s t-distribution and Inference of the Population Mean"
author: "Jeffrey Arnold"
date: "February 10, 2016"
output: html_document
---

```{r}
library("dplyr")
set.seed(153305)
```

The Central Limit Theorem states that the sampling distribution of the sample 
mean is $N(\mu, \sigma / \sqrt{n})$ as $n$ gets large.
This means that test-statistics and critical values are defined as,
$$
\frac{\bar{x} - \mu}{\sigma / \sqrt{n}} \sim N(0, 1)
$$
However, the previous equation assumes that the population standard deviation, $\sigma$, is known.
It would be unusual to have an unknown population mean, but a known population standard deviation.
In practice, the test statistic that is used plugs in the sample standard deviation for the population standard deviation,
$$
\frac{\bar{x} - \mu}{\sigma / \sqrt{n}} \approx \frac{\bar{x} - \mu}{s / \sqrt{n}}
$$
But the sample variance, and by extension, the sample standard deviation, will vary across samples,
```{r}
for (i in 1:5) {
  var(rnorm(10))
}
```
So the sampling distribution of the test statistic of the mean will need to account for both the variability of sample means and standard deviations.

# Sampling distribution of the sample variances

To get some intuition about the sampling distribution of the variance, draw samples from the standard normal distribution at several sample sizes, and save the mean and variance of each sample.
```{r}
sample_sizes <- as.integer(c(4, 16, 32, 256))
samples <-
    data_frame(size = sample_sizes,
               iter = 2048) %>%
    group_by(size) %>%
    do({
      results <- list()
      for (i in 1:.$iter) {
        x <- rnorm(.$size)
        results[[i]] <- data_frame(x_mean = mean(x),
                                   x_var = var(x))
      }
      bind_rows(results)
    }) %>% ungroup()
```

```{r}
ggplot(samples, aes(x = x_var, colour = factor(size))) +
  geom_density()
```
When the population distribution is standard normal, the sampling distribution of the sum of squares ($n - 1 s ^ 2 = \sum (x_i - \bar{x})^2$) from independent samples is a Chi-squared distribution,
$$
(n - 1) s^2  / \sigma^2 \sim \chi^2_{n - 1}
$$
```{r}
samples_w_chisq <- samples %>%
  mutate(sum_sq = x_var * (size - 1),
         chisq = dchisq(x_var * size, df = size),
         size = factor(size))
ggplot(samples_w_chisq) +
  geom_density(mapping = aes(x = sum_sq), color = "black") +
  geom_line(mapping = aes(x = sum_sq, y = chisq), color = "red") +
  facet_wrap(~size, scales = "free_x")
```

How does the sampling variability of the standard deviation affect the sampling variability of the mean test statistics?
The tails of the distribution of $(\bar{x} - \mu) / (s / \sqrt{n})$ are wider than those of $(\bar{x} - \mu) / (\sigma / \sqrt{n})$.
At smaller sample sizes, when there is more variance in the sum of squares, this difference is noticeable, but as the sample sizes increases, the tails become narrower, as the standard error of the standard deviation decreases.
```{r}
comparison <- samples %>%
  mutate(known = x_mean / (1 / sqrt(size)),
         estimated = x_mean / sqrt(x_var / size)) %>%
  select(size, known, estimated) %>%
  gather(sigma, value, -size)
ggplot(comparison, aes(samples = value, colour = sigma)) +
  geom_qq() +
  facet_wrap(~size, scales = "free")
```

Student's $t$-distribution is the distribution of the sampling distribution of the mean with an unknown standard deviation.


TODO: coverage ratios of confidence intervals with and without Student's t distribution

TODO: Type I errors for sample sizes with normal and Student's t distribution


