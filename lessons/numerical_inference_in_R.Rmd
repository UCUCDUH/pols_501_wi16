---
title: "Numerical Inference in R"
author: "Jeffrey Arnold"
date: "February 10, 2016"
---

This lesson introduces the R commands for basic numerical inference.

- `t.test()`: One-sample mean, two-sample differenc in means, and paired difference in means confidence intervals and hypothesis tests
- `anova()`: ANOVA
- `pairwise.t.test()`: Pairwise t-tests adjusting p-value for multiple comparisons

This lesson uses the following packages, although all the functions for inference (`t.test()`, `anova()`, and `pairwise.t.test()`) are included with R.
```{r message=FALSE}
library("car")
library("broom")
library("dplyr")
library("ggplot2")
library("tidyr")
```

This example will use Duncan's occupational prestige data from the **car** package,
which has the prestige and other characteristics of 45 U.S. occupations in 1950.
```{r}
data("Duncan")
glimpse(Duncan)
```

This lesson will also use Student's sleep data.
This is a dataset on the effect of two different sleep medications on 10 patients that was used in Student's 1908 paper.
```{r}
data("sleep")
glimpse(sleep)
```
The variable `group` refers to the medication, and not a treatment/control group. 
In the experiment, 10 patients each took both drugs, and the increase in hours of sleep (relative to their baseline) was measured in `extra`.


# t-test

The R function `t.test()` calculates both hypothesis tests and confidence intervals for one- and two-sample t-tests, and paired t-tests.

In the Duncan data, the `prestige` variable is the percent of raters which rate the occupation as "excellent" or "good".
The average prestige score is
```{r}
summary(Duncan$prestige)
```
Now analyze the mean of `prestige` using `t.test()`,
```{r}
t.test(Duncan$prestige)
```
By default `t.test()` runs a two-sided hypothesis test where $H_0: \mu = 0$ and $H_a: \mu \neq 0$,
and calculates a 95% confidence interval.

To calculate a different hypothesis test, use the options `mu` (to change the null hypothesis parameter value), and `alternative` to choose a one-sided ("less", "greater") instead of a two-sided test.
This runs a two-sided test where $H_0: \mu = 50$,
```{r}
t.test(Duncan$prestige, mu = 50)
```
This runs a one-sided test where $H_0: \mu = 50$ and $H_a: \mu < 50$,
```{r}
t.test(Duncan$prestige, mu = 50, alternative = "less")
```
This runs a one-sided test where $H_0: \mu = 45$ and $H_a: \mu > 45$,
```{r}
t.test(Duncan$prestige, mu = 45, alternative = "greater")
```

By default `t.test()` calculates a 95% confidence interval. To change the confidence level, use the `conf.level` argument.
This calculates a 99% confidence interval,
```{r}
t.test(Duncan$prestige, conf.level = 0.99)
```
and this calculates a 90% confidence interval,
```{r}
t.test(Duncan$prestige, conf.level = 0.90)
```

Instead of printing the results of `t.test()` to a console, they can be saved to a variable and used for later computations.
```{r}
prestige_ttest <- t.test(Duncan$prestige, mu = 50)
```
This object can be printed to the console,
```{r}
prestige_ttest
```
However, it can also be used in computations.
The p-value can be extracted using `$`,
```{r}
prestige_ttest$p.value
```
To extract the confidence interval,
```{r}
prestige_ttest$conf.int
```
and the test statistic,
```{r}
prestige_ttest$statistic
```
The full set of elements that can be extracted from the results of `t.test()` can be listed with `names()`,
```{r}
names(prestige_ttest)
```
See the documantation of `t.test()` for what they do.

The **broom** package provides another interface to the results of `t.test()` (and many other statistics models).
The function `tidy()` from this package will return a data frame from the results of `t.test()`,
```{r}
tidy(prestige_ttest)
```
While the data frame returned by `tidy()` will not be as detailed as the original object generated by a statistical model, as a data frame, it can be much more convenient to work with in some contexts (see the example below).

In addition to one-sample t-tests, `t.test()` also can estimate two-sample (difference in means) tests and confidence intervals.
To compare the mean prestige of blue-collar ("bc") and professional ("prof") occupations,
```{r}
t.test(filter(Duncan, type == "bc")$prestige,
       filter(Duncan, type == "prof")$prestige)
```
Now `t.test()` has two arguments; the first two vectors are the values in the two samples.
You can also use a formula notation with `t.test`.
This does the same thing as the previous code,
```{r}
t.test(prestige ~ type, data = filter(Duncan, type %in% c("prof", "bc")))
```
In the expression `prestige ~ type`, the left-hand side indicates the numeric vector (`prestige`) and the right-hand side variable indicates the variable containing the categories (`type`).
The argument `data` indicates where to look for the `prestige` and `type` variables.
In this test, the Duncan data is filtered to only include the observations with categories "prof" and "bc" since those are the only observations used in calculating the difference in means.
If you try to run `t.test()` with a grouping factor with more than two levels, it will throw an error,
```{r error = TRUE}
t.test(prestige ~ type, data = Duncan)
```

Now, suppose we wanted to use `t.test()` to find a 95% confidence interval of the mean of  occupational prestige for each category of occpuational type (blue-collar, white-collar, and professional).
One way to do this would be to run each test separately,
```{r}
t.test(filter(Duncan, type == "bc")$prestige)
t.test(filter(Duncan, type == "wc")$prestige)
t.test(filter(Duncan, type == "prof")$prestige)
```
This code could be simplified by using a `for` loop,
```{r}
types <- c("bc", "wc", "prof")
for (typ in types) {
  print(t.test(filter(Duncan, type == typ)$prestige))
}
```
Note that in a `for` loop you need to explicitly specify `print()`.
However, if we wanted to put the results of these tests in a table or plots, or otherwise use them in future computations, we still would have to do extra processing to extract values from each test and save them to other variables.

Using **dplyr** and **broom** can simplify that task.
The following code runs `t.test()` on each category of `type`, converts the results of the t-test to a data frame using `tidy`.
This areturns a data frame with a row for each catgory of `type`, and columns with results from the t-tests,
```{r}
duncan_t_tests <-
  Duncan %>%
    group_by(type) %>%
    do(tidy(t.test(.$prestige)))
duncan_t_tests
```
This can be used to plot the confidence intervals,
```{r}
ggplot(duncan_t_tests, aes(x = type,
                           y = estimate,
                           ymin = conf.low,
                           ymax = conf.high)) +
  geom_pointrange() +
  coord_flip() +
  xlab("Occupation Type") +
  ylab("Occupation Prestige")
```
or p-values
```{r}
ggplot(duncan_t_tests, aes(x = type,
                           y = p.value)) +
  geom_point() +
  coord_flip() +
  ylab("p-value") +
  xlab("Occupation Type")
```


Now, consider the sleep data. 
Since it has the extra hours of sleep for the same set of 10 patients for two drugs, this is paired data, and should be analyzed using a paired t-test.
Although the data is paired, `t.test()` does not know this and will run a two-sample difference in means test by default,
```{r}
t.test(extra ~ group, data = sleep)
```
The appropriate, paired t-test is,
```{r}
t.test(extra ~ group, data = sleep, paired = TRUE)
```
This could also be done manually.
Create a wide data frame in which `med1` and `med2` are columns with the extra hours of sleep for each drug, for each patient.
Then create a column `diff` which is the difference in the extra hours of sleep for each drug.
```{r}
sleep_wide <-
  sleep %>%
  mutate(group = paste("med", group, sep = "")) %>%
  spread(group, extra) %>%
  mutate(diff = med1 - med2)
glimpse(sleep_wide)
```
Running a one-sample t-test on the difference in the extra hours of sleep for each patience gives the same results as using `paired = TRUE` on the original data,
```{r}
t.test(sleep_wide$diff)
```


TODO: Examples using data from OpenIntro Numeric Inference Lab.


# ANOVA

One way to run a one-way ANOVA is to run a linear regression in R using `lm()` with a single categorical explanatory variable, and then apply `anova()` to the result.
```{r}
duncan_type_anova <- anova(lm(prestige ~ type, data = Duncan))
duncan_type_anova
```

In `R`, the use of `~` is known as the formula notation.
It can have slightly different meanings in different functions, but in `lm()`, the left-hand variable is the outcome variable, and the right-hand side variable(s) is (are) the explanatory variable(s). 

# Pairwise t-tests

The function `pairwise.t.test` does what it says; it runs pairwise t-tests.
The first argument is a numeric vector; the second argument is the vector with categories.
The argument `p.adjust.method` allows the user to choose a method to adjust the p-values for multiple comparison. 

The Bonferroni correction adjusts the p-values so that the adjusted p-value is $\bar{alpha} = \alpha / m$, where $m$ is the number of tests. It controls the Familywise Error Rate (FWER).
This is the probability that there is *at least one* Type I error,
```{r}
pairwise.t.test(Duncan$prestige, Duncan$type, p.adjust.method = "bonferroni")
```
The "Holm" correction, like the Bonferroni correction, controls the FWER, but is more powerful than the Bonferroni correction.
```{r}
pairwise.t.test(Duncan$prestige, Duncan$type, p.adjust.method = "holm")
```
An alternative approach to correcting multiple comparisons is the False Discovery Rate (FDR).
The False Discovery Rate is the expected rate of false positives among all positives.
```{r}
pairwise.t.test(Duncan$prestige, Duncan$type, p.adjust.method = "BH")
```
See [EGAP](http://egap.org/methods-guides/10-things-you-need-know-about-multiple-comparisons) for more discussion of multiple testing adjustment.

The **broom** package has some support for `pairwise.t.test`, and will put 
```{r}
pw_ttest_Duncan_type <- pairwise.t.test(Duncan$prestige, Duncan$type)
tidy(pw_ttest_Duncan_type)
```

**Note:** Pairwise t-tests are different than paired t-tests. In a paired t-test there are two populations which are not independent, in which each observation in one sample can be matched to another observation in the other sample. An example of this would be a before-after study.
Pairwise t-tests are used to compare each pair of $K \geq 2$ samples, for a total of $K (K + 1) / 2$ difference in means t-tests. Since there are multiple t-tests, the p-values of pairwise t-tests need to be corrected for multiple testing.
